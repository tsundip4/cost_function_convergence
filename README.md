The goal is to observe how the cost function decreases with each iteration and identify when the model converges during gradient descent optimization.

Convergence Observation
Plotted Cost vs Iteration.
The cost decreased rapidly at first and gradually flattened after ~1300 iterations, indicating convergence.
The plot shows the cost value logged every 50 iterations.

Results:
The cost function decreased smoothly and stabilized, confirming proper convergence.
The learning rate chosen allows a steady and monotonic decline in cost.
The final parameters (w_final, b_final) give the best-fit line for predicting house value based on house age.
